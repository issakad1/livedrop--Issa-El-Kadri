# -*- coding: utf-8 -*-
"""chat-interface

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n1xGn5FsgEVH9l1d9ImSZstTiMj5I0Xb
"""

"""
Shoplite RAG CLI Client
A command-line interface for interacting with the deployed Shoplite LLM chatbot.
"""
import requests
import json
import time
from datetime import datetime
from typing import Optional, Dict, Any
import os


class ShopliteRAGClient:
    """Client for interacting with the Shoplite RAG API."""

    def __init__(self, base_url: str, log_file: str = "conversation_log.jsonl", debug: bool = False):
        """
        Initialize the client.

        Args:
            base_url: The ngrok tunnel URL (e.g., https://xxxx.ngrok.io)
            log_file: Path to the conversation log file
            debug: If True, show raw LLM responses
        """
        self.base_url = base_url.rstrip('/')
        self.log_file = log_file
        self.conversation_history = []
        self.debug = debug

    def check_health(self) -> bool:
        """Check if the API is healthy and responsive."""
        try:
            response = requests.get(
                f"{self.base_url}/health",
                timeout=10
            )
            if response.status_code == 200:
                data = response.json()
                print(f"✓ API is healthy")
                print(f"✓ Documents loaded: {data.get('documents_loaded', 'Unknown')}")
                return True
            else:
                print(f"✗ API returned status code: {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            print(f"✗ Connection failed: {e}")
            return False

    def _clean_answer(self, raw_answer: str) -> str:
        """
        Clean the LLM answer by removing unwanted prefixes and explanations.

        Args:
            raw_answer: Raw answer from the LLM

        Returns:
            Cleaned answer text
        """
        answer = raw_answer.strip()

        # Pattern 1: Remove everything from "Explanation:" onwards
        if "Explanation:" in answer:
            answer = answer.split("Explanation:")[0].strip()

        # Pattern 2: If the answer is ONLY meta-commentary, return a placeholder
        lower_answer = answer.lower()
        meta_only_phrases = [
            "the provided documents are",
            "the answer should be based on",
            "the final answer should be",
            "including the document titles"
        ]

        if any(phrase in lower_answer for phrase in meta_only_phrases):
            # Check if there's ANY substantive content
            lines = answer.split('\n')
            substantive_lines = []

            for line in lines:
                line_clean = line.strip()
                line_lower = line_clean.lower()

                # Skip lines that are pure meta-commentary
                is_meta = any(phrase in line_lower for phrase in [
                    "provided documents",
                    "based on the information",
                    "the answer should",
                    "final answer should",
                    "documents are the",
                    "response should be",
                    "including the document"
                ])

                # Skip bullet points (-)
                is_bullet = line_clean.startswith('-')

                # Keep lines that have actual content
                if line_clean and not is_meta and not is_bullet:
                    substantive_lines.append(line_clean)

            if substantive_lines:
                answer = ' '.join(substantive_lines)
            else:
                # No substantive content found - the LLM failed to answer
                return "I apologize, but I wasn't able to find a specific answer to your question in the Shoplite documentation. Could you rephrase your question or be more specific?"

        # Pattern 3: Remove leading/trailing meta phrases
        answer = answer.strip()

        # Final cleanup: remove any remaining standalone bullets at start
        while answer and answer[0] in ['-', '*', '•']:
            answer = answer[1:].strip()

        return answer if answer else raw_answer.strip()

    def ask_question(
        self,
        query: str,
        prompt_type: str = "base_retrieval_prompt",
        timeout: int = 120
    ) -> Optional[Dict[str, Any]]:
        """
        Send a question to the LLM and get a response.

        Args:
            query: The user's question
            prompt_type: Type of prompt to use
            timeout: Request timeout in seconds

        Returns:
            Dictionary with answer, sources, and metadata or None if failed
        """
        print("[Retrieving context...]")

        try:
            payload = {
                "query": query,
                "prompt_type": prompt_type
            }

            start_time = time.time()
            print("[Calling LLM...]")

            response = requests.post(
                f"{self.base_url}/chat",
                json=payload,
                timeout=timeout
            )

            elapsed_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()

                # Clean the answer
                raw_answer = data.get("answer", "")
                cleaned_answer = self._clean_answer(raw_answer)

                # Add metadata
                result = {
                    "query": query,
                    "answer": cleaned_answer,
                    "raw_answer": raw_answer,  # Keep original for debugging
                    "sources": data.get("sources", []),
                    "prompt_type": prompt_type,
                    "response_time": round(elapsed_time, 2),
                    "timestamp": datetime.now().isoformat(),
                    "confidence": self._estimate_confidence(data)
                }

                # Log the conversation
                self._log_conversation(result)
                self.conversation_history.append(result)

                return result
            else:
                error_data = response.json() if response.content else {}
                print(f"✗ API Error {response.status_code}: {error_data.get('error', 'Unknown error')}")
                return None

        except requests.exceptions.Timeout:
            print(f"✗ Request timed out after {timeout} seconds")
            return None
        except requests.exceptions.RequestException as e:
            print(f"✗ Connection error: {e}")
            return None
        except Exception as e:
            print(f"✗ Unexpected error: {e}")
            return None

    def _estimate_confidence(self, data: Dict[str, Any]) -> str:
        """
        Estimate confidence level based on response characteristics.

        Args:
            data: Response data from API

        Returns:
            Confidence level string
        """
        answer = data.get("answer", "").lower()
        sources = data.get("sources", [])

        # Low confidence indicators
        if "don't know" in answer or "not specified" in answer or not sources:
            return "Low"

        # Medium confidence
        if len(sources) == 1:
            return "Medium"

        # High confidence
        if len(sources) >= 2:
            return "High"

        return "Medium"

    def _log_conversation(self, result: Dict[str, Any]) -> None:
        """
        Log the conversation to a JSONL file.

        Args:
            result: Conversation result to log
        """
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(result) + '\n')
        except Exception as e:
            print(f"Warning: Failed to log conversation: {e}")

    def display_response(self, result: Dict[str, Any]) -> None:
        """
        Display the response in a formatted way.

        Args:
            result: Response data to display
        """
        print("\n" + "="*60)
        print(f"Answer: {result['answer']}")

        # Show raw answer in debug mode
        if self.debug and result.get('raw_answer') != result['answer']:
            print("\n[DEBUG] Raw LLM output:")
            print(f"{result.get('raw_answer', '')}")

        print("-"*60)
        print(f"Sources: {', '.join(result['sources']) if result['sources'] else 'None'}")
        print(f"Confidence: {result['confidence']}")
        print(f"Response time: {result['response_time']}s")
        print("="*60 + "\n")

    def show_history(self, limit: int = 5) -> None:
        """
        Display recent conversation history.

        Args:
            limit: Number of recent conversations to show
        """
        if not self.conversation_history:
            print("No conversation history yet.")
            return

        print("\n" + "="*60)
        print("RECENT CONVERSATION HISTORY")
        print("="*60)

        for i, conv in enumerate(self.conversation_history[-limit:], 1):
            print(f"\n[{i}] Q: {conv['query']}")
            print(f"    A: {conv['answer'][:100]}{'...' if len(conv['answer']) > 100 else ''}")
            print(f"    Sources: {', '.join(conv['sources'][:2])}")

        print("="*60 + "\n")


def print_banner():
    """Print welcome banner."""
    print("\n" + "="*60)
    print("   SHOPLITE RAG CHATBOT - Command Line Interface")
    print("="*60)
    print("Type your questions about Shoplite policies and features.")
    print("Commands:")
    print("  - 'quit' or 'exit': Exit the program")
    print("  - 'history': Show recent conversation history")
    print("  - 'clear': Clear the screen")
    print("  - 'help': Show available prompt types")
    print("  - 'debug on/off': Toggle debug mode (show raw LLM output)")
    print("  - 'raw': Show raw output of last response")
    print("="*60 + "\n")


def print_help():
    """Print help information about prompt types."""
    print("\nAvailable prompt types:")
    print("  - base_retrieval_prompt (default): Standard Q&A")
    print("  - multi_document_prompt: Combine info from multiple docs")
    print("  - numeric_truth_prompt: Precise numbers and timeframes")
    print("  - policy_plus_procedure_prompt: Policy rules and steps")
    print("  - clarification_prompt: Ask for clarification")
    print("\nTo use a specific prompt type, prefix your question with:")
    print("  [prompt_type] your question here")
    print("Example: [numeric_truth_prompt] What is the return window?\n")


def main():
    """Main CLI loop."""
    print_banner()

    # Get ngrok URL from user
    ngrok_url = input("Enter your ngrok tunnel URL (e.g., https://xxxx.ngrok.io): ").strip()

    if not ngrok_url:
        print("Error: URL cannot be empty")
        return

    # Initialize client
    client = ShopliteRAGClient(ngrok_url)

    # Check health
    print("\nChecking API health...")
    if not client.check_health():
        print("\nFailed to connect to API. Please check your ngrok URL and try again.")
        return

    print("\n✓ Connected successfully! You can start asking questions.\n")

    # Main loop
    while True:
        try:
            # Get user input
            user_input = input("> ").strip()

            if not user_input:
                continue

            # Handle commands
            if user_input.lower() in ['quit', 'exit']:
                print("\nGoodbye! Your conversation has been logged to:", client.log_file)
                break

            elif user_input.lower() == 'history':
                client.show_history()
                continue

            elif user_input.lower() == 'clear':
                os.system('cls' if os.name == 'nt' else 'clear')
                print_banner()
                continue

            elif user_input.lower() == 'help':
                print_help()
                continue

            elif user_input.lower().startswith('debug'):
                if 'on' in user_input.lower():
                    client.debug = True
                    print("✓ Debug mode enabled - raw LLM outputs will be shown\n")
                elif 'off' in user_input.lower():
                    client.debug = False
                    print("✓ Debug mode disabled\n")
                else:
                    status = "ON" if client.debug else "OFF"
                    print(f"Debug mode is currently {status}")
                    print("Use 'debug on' or 'debug off' to change\n")
                continue

            elif user_input.lower() == 'raw':
                if client.conversation_history:
                    last = client.conversation_history[-1]
                    print("\n" + "="*60)
                    print("RAW LLM OUTPUT (last response):")
                    print("="*60)
                    print(last.get('raw_answer', 'No raw output available'))
                    print("="*60 + "\n")
                else:
                    print("No conversation history yet.\n")
                continue

            # Parse prompt type if specified
            prompt_type = "base_retrieval_prompt"
            query = user_input

            if user_input.startswith('['):
                end_bracket = user_input.find(']')
                if end_bracket != -1:
                    prompt_type = user_input[1:end_bracket].strip()
                    query = user_input[end_bracket+1:].strip()

            # Ask question
            result = client.ask_question(query, prompt_type=prompt_type)

            if result:
                client.display_response(result)
            else:
                print("Failed to get response. Please try again.\n")

        except KeyboardInterrupt:
            print("\n\nInterrupted. Type 'quit' to exit gracefully.")
        except Exception as e:
            print(f"\nError: {e}")
            print("Please try again.\n")


if __name__ == "__main__":
    main()